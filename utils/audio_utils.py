import wave
import os
import numpy as np
import soundfile as sf
from pydub import AudioSegment
import librosa

print("ğŸ¯ audio_utils path:", __file__)

def light_analyze(wav_path):
    """
    â‘ ã€œâ‘¢ ã®è¶…è»½é‡è§£æã ã‘ã‚’è¡Œã„ã€
    (score:int, is_fallback:bool) ã‚’è¿”ã™
    """
    # WAVèª­ã¿è¾¼ã¿ï¼šsoundfile â†’ pydub ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    try:
        y, sr = sf.read(wav_path, dtype='float32')
    except Exception:
        audio = AudioSegment.from_file(wav_path)
        arr = np.array(audio.get_array_of_samples()).astype(np.float32)
        sr = audio.frame_rate
        if audio.channels == 2:
            arr = arr.reshape((-1, 2)).mean(axis=1)
        y = arr

    duration = len(y) / sr
    abs_y = np.abs(y)
    if duration < 1.5 or np.mean(abs_y < 0.01) > 0.95:
        return 40, True

    # â‘  å£°é‡å¤‰å‹•ï¼ˆæŒ¯å¹…STDï¼‰
    vol_std = float(np.std(abs_y))
    # â‘¡ æœ‰å£°éŸ³ç‡ï¼ˆå˜ç´”é–¾å€¤ï¼‰
    voiced_ratio = float((abs_y > 0.02).sum()) / len(abs_y)
    # â‘¢ ã‚¼ãƒ­äº¤å·®ç‡ï¼ˆé«˜é€Ÿï¼‰
    zero_crossings = ((y[:-1] * y[1:]) < 0).sum()
    zcr = float(zero_crossings) / len(y)

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° 0â€“100
    vol_s = np.clip(vol_std * 1000, 0, 100)
    voice_s = np.clip(voiced_ratio * 100, 0, 100)
    zcr_s = np.clip(zcr * 100, 0, 100)

    raw = vol_s * 0.4 + voice_s * 0.4 + zcr_s * 0.2
    score = round(np.clip(raw, 30, 95))
    return score, False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WAV å¤‰æ›ç³» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def convert_webm_to_wav(input_path, output_path):
    audio = AudioSegment.from_file(input_path, format="webm")
    audio.export(output_path, format="wav")

def convert_m4a_to_wav(input_path, output_path):
    import subprocess
    subprocess.run([
        'ffmpeg', '-y', '-i', input_path,
        '-acodec', 'pcm_s16le', '-ac', '1', '-ar', '44100',
        '-f', 'wav', output_path
    ], check=True)
    print("âœ… ffmpegå¤‰æ›æˆåŠŸ")

def normalize_volume(input_path, output_path, target_dBFS=-5.0):
    audio = AudioSegment.from_file(input_path)
    diff = target_dBFS - audio.dBFS
    normalized = audio.apply_gain(diff)
    normalized.export(
        output_path, format="wav",
        parameters=['-acodec','pcm_s16le','-ar','44100','-ac','1']
    )

def is_valid_wav(wav_path, min_duration_sec=1.5):
    try:
        with wave.open(wav_path, 'rb') as wf:
            return wf.getnframes() / wf.getframerate() >= min_duration_sec
    except Exception as e:
        print("âŒ WAVæ¤œè¨¼ã‚¨ãƒ©ãƒ¼:", e)
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ãƒ•ãƒ«è§£æï¼ˆâ‘ ã€œâ‘¤ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_stress_from_wav(wav_path, user_id=None):
    """
    return (score:int, is_fallback:bool)
    30â€“95 ç‚¹ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    """
    try:
        y, sr = sf.read(wav_path, dtype='float32')
        if y.ndim == 2:
            y = y.mean(axis=1)
        if y.size == 0:
            raise ValueError("empty")

        duration = len(y) / sr
        abs_y = np.abs(y)
        if duration < 1.5 or np.mean(abs_y < 0.01) > 0.95:
            return 50, True

        # â‘  å£°é‡å¤‰å‹•
        volume_std = float(np.std(abs_y))
        # â‘¡ ç²¾å¯† Voiced ç‡
        intervals = librosa.effects.split(y, top_db=40)
        voiced_dur = sum(e - s for s, e in intervals) / sr
        voiced_ratio = voiced_dur / duration
        # â‘¢ ã‚¼ãƒ­äº¤å·®ç‡
        zcr = float(librosa.feature.zero_crossing_rate(
            y, frame_length=2048, hop_length=512).mean())
        # â‘£ ãƒ”ãƒƒãƒæ¨™æº–åå·®
        pitches, mags = librosa.piptrack(y=y, sr=sr)
        p = pitches[mags > np.median(mags)]
        pitch_std = float(np.std(p)) if p.size else 0.0
        # â‘¤ ãƒ†ãƒ³ãƒ
        onset = librosa.onset.onset_detect(y=y, sr=sr, units='frames')
        times = librosa.frames_to_time(onset, sr=sr)
        tempo_val = len(times)/(times[-1]-times[0]) if len(times)>1 else 0.0

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        vol_scaled = np.clip(volume_std * 7000, 0, 100)
        voice_scaled = np.clip(voiced_ratio * 150, 0, 100)
        zcr_scaled = np.clip(zcr * 7000, 0, 100)
        pitch_scaled = np.clip(pitch_std * 0.3, 0, 100)
        tempo_scaled = 100 - np.clip(abs(tempo_val - 4.5) * 8, 0, 100)

        raw = (vol_scaled * 0.25 + voice_scaled * 0.2 +
               zcr_scaled * 0.2 + pitch_scaled * 0.2 + tempo_scaled * 0.15)
        score = round(np.clip(raw, 25, 97))

        user = User.query.get(user_id)
        if user and user.volume_baseline:
            if volume_std < user.volume_baseline * 0.5:
                print("âš ï¸ å£°é‡ãŒæ¥µç«¯ã«ä½ä¸‹ â†’ æ¸›ç‚¹è£œæ­£")
                score = max(25, score - 10)

        print(f"ğŸ“Š ã‚¹ã‚³ã‚¢æ§‹æˆ: vol={vol_scaled:.1f}, voice={voice_scaled:.1f}, "
              f"zcr={zcr_scaled:.1f}, pitch={pitch_scaled:.1f}, tempo={tempo_scaled:.1f} "
              f"â†’ raw={raw:.1f} â†’ score={score}")

        # ğŸ”» å˜èª¿ã™ããƒ»å°å£°ã™ãã¸ã®ç½°å‰‡
        if volume_std < 0.003 or pitch_std < 0.5 or tempo_val < 0.5:
            print("âš ï¸ è©±ã—æ–¹ãŒå˜èª¿ã¾ãŸã¯å£°é‡ãŒæ¥µç«¯ã«å°ã•ã„ãŸã‚è£œæ­£ã‚¹ã‚³ã‚¢é©ç”¨")
            return {
                "score": 30 + np.random.randint(0, 10),
                "is_fallback": True,
                "volume_std": volume_std,
                "voiced_ratio": voiced_ratio,
                "zcr": zcr,
                "pitch_std": pitch_std,
                "tempo_val": tempo_val,
            }

        return {
            "score": score,
            "is_fallback": False,
            "volume_std": volume_std,
            "voiced_ratio": voiced_ratio,
            "zcr": zcr,
            "pitch_std": pitch_std,
            "tempo_val": tempo_val,
        }

    except Exception as e:
        print("âŒ analyze error:", e)
        return {
            "score": 50,
            "is_fallback": True,
            "volume_std": None,
            "voiced_ratio": None,
            "zcr": None,
            "pitch_std": None,
            "tempo_val": None,
        }
